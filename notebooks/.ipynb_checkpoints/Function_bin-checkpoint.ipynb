{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporary notebook to collect up the functions that I'll use later when I make this official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os, sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import GetOldTweets3 as got\n",
    "\n",
    "def scrape_new_tweets(t_last_tweet,username = \"elonmusk\"):\n",
    "    \"\"\"Function to scrape the recent tweets of Elon Musk\"\"\"\n",
    "    #t_last_tweet must be pandas Timestamp data\n",
    "    os.makedirs('tweet_data', exist_ok=True)\n",
    "    date_str = str(t_last_tweet.date().year)+\"-\"\\\n",
    "              +str(t_last_tweet.date().month)+\"-\"\\\n",
    "              +str(t_last_tweet.date().day)\n",
    "    count = 0\n",
    "    # Creation of query object                                                                                                                                                                                      \n",
    "    tweetCriteria = got.manager.TweetCriteria().setUsername(username)\\\n",
    "                                               .setMaxTweets(count)\\\n",
    "                                               .setSince(date_str)\n",
    "    # Creation of list that contains all tweets                                                                                                                                                                     \n",
    "    tweets = None\n",
    "    for ntries in range(5):\n",
    "        try:\n",
    "            tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "        except SystemExit:\n",
    "            print(\"Trying again in 15 minutes.\")\n",
    "            time.sleep(15*60)\n",
    "        else:\n",
    "            break\n",
    "    if tweets is None:\n",
    "        print(\"Failed after 5 tries, quitting!\")\n",
    "        exit(1)\n",
    "\n",
    "    data = defaultdict(list)\n",
    "    for t in tweets:\n",
    "        data[\"username\"].append(username)\n",
    "        data[\"tweet_id\"].append(t.id)\n",
    "        data[\"reply_to\"].append(t.to)\n",
    "        data[\"date\"].append(t.date)\n",
    "        data[\"retweets\"].append(t.retweets)\n",
    "        data[\"favorites\"].append(t.favorites)\n",
    "        data[\"hashtags\"].append(list(set(t.hashtags.split())))\n",
    "        data[\"mentions\"].append(t.mentions)\n",
    "        data[\"text\"].append(t.text)\n",
    "        data[\"permalink\"].append(t.permalink)\n",
    "    if len(data) == 0: #no new tweets\n",
    "        return None\n",
    "    else:\n",
    "        #make a DataFrame out of the scraped tweets\n",
    "        df = pd.DataFrame(data, columns=[\"username\",\"tweet_id\",\n",
    "                                         \"reply_to\",\"date\",\"retweets\",\n",
    "                                         \"favorites\",\"hashtags\",\"mentions\",\n",
    "                                         \"text\",\"permalink\"])        \n",
    "        # Convert 'Time' column to datetime and strip time information.\n",
    "        df['Time'] = pd.to_datetime(df['date']).sort_values(by='Time',ascending=True)\n",
    "        return df\n",
    "    \n",
    "def reload_tweet_data(username=\"elonmusk\",path):\n",
    "   #note we'll have to do a .drop and set the 'Time' column to the proper values every time\n",
    "    df = pd.read_csv(path+username+'.csv').drop(['Unnamed: 0'],axis='columns')\n",
    "    #order by earliest first\n",
    "    df['Time'] = pd.to_datetime(df['Time'])#.sort_values(by='Time',ascending=True)\n",
    "    return df.sort_values(by='Time',ascending=True).reset_index().drop('index',axis='columns')\n",
    "\n",
    "def prepend_new_tweets(df_new,df_old): #adds the new tweets to the front of the data set and resets the index\n",
    "    result = pd.concat([df_old,df_new]).reset_index().drop('index',axis='columns')\n",
    "    #makes sure we're sorted properly in time order\n",
    "    result.sort_values(by='Time',ascending=True)\n",
    "    return result.reset_index().drop('index',axis='columns')\n",
    "\n",
    "def store_tweet_data(df,username=\"elonmusk\",path):\n",
    "    df.to_csv(path+username+'.csv')\n",
    "    return\n",
    "\n",
    "def scan_for_new_tweets(username=\"elonmusk\",path):\n",
    "    df_old = reload_tweet_data(username=\"elonmusk\",path) #get the old tweets\n",
    "    #look for new tweets starting from latest date\n",
    "    df_new = scrape_new_tweets(df_old['Time'].max(),username = \"elonmusk\")\n",
    "    if df_new == None:# No new tweets\n",
    "        return df_old\n",
    "    else:\n",
    "        df_combined = prepend_new_tweets(df_new,df_old)\n",
    "        return df_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/JJ/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "nltk.download('vader_lexicon') #get the bloody lexicon\n",
    "\n",
    "\n",
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"RT\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),:;!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df = df[df[text_field].notna()]\n",
    "    return df\n",
    "\n",
    "def apply_vader(tweet,category): #gives back the float value of the vader sentiment\n",
    "    return sid.polarity_scores(tweet)[category]\n",
    "\n",
    "def tokenize_hashtags(df): #tokenize hashtags, using one hot encoding\n",
    "    df['hashtags_token'] = 0. #initialize all to zero\n",
    "    df['hashtags_token'].loc[df['hashtags'] != '[]'] = 1. #any field with a hashtag set to 1.\n",
    "    return df\n",
    "\n",
    "def tokenize_mentions(df): #tokenize mentions, using one hot encoding\n",
    "    df['mentions_token'] = 0. #initialize all to zero\n",
    "    df['mentions_token'].loc[df['mentions'].notna()] = 1. #any field with a mention set to 1.\n",
    "    return df\n",
    "\n",
    "def tokenize_reply_to(df): #tokenize mentions, using one hot encoding\n",
    "    df['reply_to_token'] = 0. #initialize all to zero\n",
    "    df['reply_to_token'].loc[df['reply_to'].notna()] = 1. #any field with a reply_to set to 1.\n",
    "    return df\n",
    "\n",
    "def convert_hashtag(input_txt):\n",
    "    if input_txt == '[]': #return empty string if no hashtag\n",
    "        return \"\"\n",
    "    input_list = input_txt.strip(\"['']\").split(\"', '\") #strips out useless characters\n",
    "    txt_list = re.findall('[A-Z][^A-Z]*', \" \".join(input_list)) #splits hastags into words on Captial letters\n",
    "    return \" \".join(txt_list)\n",
    "\n",
    "def tweet_word_count(df):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') #split on words\n",
    "    df[\"tokens\"] = df[\"tweet\"].apply(tokenizer.tokenize) #returns list of individual words\n",
    "    df['tweet_length'] = df.apply(lambda row : len(row['tokens']), axis=1) #creates tweet length column\n",
    "    df = df.drop(['tokens'],axis='columns') #drops the temporary column\n",
    "    return df   \n",
    "\n",
    "def integral_history(df,category,length):\n",
    "    #the depth back in tweet history\n",
    "    result = df[category]\n",
    "    \n",
    "    \n",
    "def construct_features(tweets):\n",
    "    \"\"\"Constructs features from Elon's tweet data\"\"\"\n",
    "    #generate the sentiment intensity analyzer instance\n",
    "    sid = SentimentIntensityAnalyzer() #returns error if no lexicon\n",
    "    # Clean the text of the tweets\n",
    "    tweets = standardize_text(tweets,\"text\")\n",
    "    # Tokenize the hashtags\n",
    "    tweets = tokenize_hashtags(tweets)\n",
    "    # Tokenize the mentions\n",
    "    tweets = tokenize_mentions(tweets)\n",
    "    # Tokenize the reply_to\n",
    "    tweets = tokenize_reply_to(tweets)\n",
    "    # Clean the text of the hastags\n",
    "    tweets[\"hashtags\"] = tweets.apply(lambda row: convert_hashtag(row['hashtags']),axis=1)\n",
    "    # Prepare to apply vader to the tweets\n",
    "    vader_categories = ['neg','neu','pos','compound']\n",
    "    # Apply vader to the tweets\n",
    "    for cat in vader_categories: #iterates over the categories\n",
    "        #creates new feature each iteration\n",
    "        tweets['text_'+cat] = tweets.apply(lambda row : apply_vader(row['text'],cat), axis=1)\n",
    "    # Apply vader to the hashtags\n",
    "    for cat in vader_categories: #iterates over the categories\n",
    "        #creates new feature each iteration\n",
    "        tweets['hashtags_'+cat] = tweets.apply(lambda row : apply_vader(row['hashtags'],cat), axis=1)\n",
    "    #Do some temporal processing\n",
    "    #Hour of the day\n",
    "    tweets['hour'] = tweets['Time'].dt.hour\n",
    "    #Time between tweets in seconds\n",
    "    tweets['delta_time'] = abs(pd.to_timedelta((tweets['Time']-tweets['Time']\\\n",
    "                                                     .shift()).fillna(6000.)).astype('timedelta64[s]'))\\\n",
    "                                                     .replace(0.,6000.)\n",
    "    tweets['log10_delta_time'] = np.log10(abs(pd.to_timedelta((tweets['Time']-tweets['Time']\\\n",
    "                                                     .shift()).fillna(60.)).astype('timedelta64[s]')\\\n",
    "                                                     .replace(0.,6000.)))\n",
    "    #Make some rate of sentiment change features\n",
    "    tweets['dcompound_dTime'] = (tweets['text_compound']-tweets['text_compound']\n",
    "                                           .shift()).fillna(0.)/(tweets['delta_time']) #change per second\n",
    "    tweets['dcompound_dTweet'] = (tweets['text_compound']-tweets['text_compound']\n",
    "                                            .shift()).fillna(0.) #change per tweet\n",
    "    #Make some integral sentiment change features\n",
    "    tweets['Integral_compound_5'] = tweets['text_compound'].rolling(min_periods=1, window=5).sum()\n",
    "    tweets['Integral_compound_10'] = tweets['text_compound'].rolling(min_periods=1, window=10).sum()\n",
    "    #Make a difference sentiment features\n",
    "    tweets['delta_compound_mean'] = tweets['text_compound'] - tweets['text_compound'].mean()\n",
    "    tweets['delta_compound_median'] = tweets['text_compound'] - tweets['text_compound'].median()\n",
    "    #All done for now\n",
    "    return tweets\n",
    "\n",
    "def strip_down_to_features_and_rescale(df):\n",
    "    #drop improperly formatted data\n",
    "    df = df.drop(['username','reply_to','retweets',\n",
    "                  'tweet_id','favorites','hashtags','mentions',\n",
    "                  'text','permalink','Time'],axis='columns')\n",
    "    # These features are on a 0 to 1 scale\n",
    "    zero_to_one = ['hour','delta_time','log10_delta_time']\n",
    "    # These features are on a -1 to 1 scale\n",
    "    negone_to_one = ['dcompound_dTime','dcompound_dTweet','integral_compound_5',\n",
    "                    'integral_compound_10','delta_compound_mean','delta_compound_median']\n",
    "    # shrink the scale for the zero_to_one features\n",
    "    zero_to_one_scale = df[zero_to_one].max()\n",
    "    df[zero_to_one] /= df[zero_to_one].max()\n",
    "    # shrink the scale for the -1 to 1 ranges\n",
    "    # need to preserve true zero, however, so no shifting the mean\n",
    "    negone_to_one_scale = [] #list to hold the rescaling\n",
    "    for x in negone_to_one:\n",
    "        # this won't fill the entire range -1 to 1, but it preserves true 0\n",
    "        df[x] /= max(abs(df[x].min()),df[x].max())\n",
    "        negone_to_one_scale += [max(abs(df[x].min()),df[x].max())]\n",
    "    return df,zero_to_one_scale,negone_to_one_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import percentile\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "from pyod.models.vae import VAE\n",
    "\n",
    "def identify_anomalies(X,outlier_fraction = 0.01,epochs=20):\n",
    "    \"\"\"A function that performs variational auto encoding analysis on the tweet data\"\"\"\n",
    "    ndim = X.shape[1] #the number of features\n",
    "    random_state = np.random.RandomState(42)\n",
    "    #outlier_fraction = 0.01 #1% of all tweets are outliers\n",
    "    #specifies the model parameters\n",
    "    classifiers = {\n",
    "        'Variational Auto Encoder (VAE)': VAE(epochs,\n",
    "                contamination = outlier_fraction, random_state = random_state,\n",
    "                encoder_neurons = [ndim,max(int(ndim/2),1),max(int(ndim/4),1)],\n",
    "                decoder_neurons = [max(int(ndim/4),1),max(int(ndim/2),1),20],\n",
    "                verbosity=0)\n",
    "    }\n",
    "\n",
    "    for i, (clf_name,clf) in enumerate(classifiers.items()):\n",
    "        clf.fit(X) #fits the model\n",
    "        scores_pred = clf.decision_function(X) * -1 #model scores\n",
    "        y_pred = clf.predict(X) #model predictions for anomalies\n",
    "    return y_pred\n",
    "\n",
    "# Don't forget to do this at some point:\n",
    "unscaled_tweet_features_df['anomalous'] = y_pred\n",
    "unscaled_tweet_features_df.to_csv('../data/processed/anomaly_tagged_tweet_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
